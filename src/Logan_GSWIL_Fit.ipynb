{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63eed960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10, FashionMNIST\n",
    "\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "\n",
    "from utils.nets import *\n",
    "from utils.model_tools import *\n",
    "from utils.dataset_tools import split_training_data\n",
    "from utils.feature_extractor import *\n",
    "from utils.cosine_similarity import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0995aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './models/'\n",
    "log_dir = './logs'\n",
    "\n",
    "model_selection = 'linear' # linear | cnn | vgg\n",
    "dataset_selection = 'fashionmnist' # cifar10 | fashionmnist\n",
    "\n",
    "ckpt_file = model_dir+'linear_fashionmnist_holdout_[8, 9].pt'\n",
    "\n",
    "holdout_classes = [8, 9]\n",
    "new_class = 8\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21ebc55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "final_learning_rate = 0.0001\n",
    "\n",
    "# initial_lr * decay_rate^num_epochs = final_lr\n",
    "decay_rate = (final_learning_rate/initial_learning_rate)**(1/num_epochs)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=initial_learning_rate)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48af967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_selection == 'fashionmnist':\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5)),]) # Images are grayscale -> 1 channel\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "825a8dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_selection == 'cifar10':\n",
    "    train_data = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    test_data = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "elif dataset_selection == 'fashionmnist':\n",
    "    train_data = FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_data = FashionMNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11b39874",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_classes = len(np.unique(train_data.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cac7277e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size 784\n",
      "num_outputs 9\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if model_selection == 'linear':\n",
    "    gswil_model = add_output_nodes(ckpt_file, arch='linear', device=device)\n",
    "    gswil_model.input_layer.requires_grad_(False)\n",
    "elif model_selection == 'cnn-demo':\n",
    "    gswil_model = add_output_nodes(ckpt_file, arch='cnn-demo', device=device)\n",
    "    gswil_model.conv1.requires_grad_(False)\n",
    "    gswil_model.conv2.requires_grad_(False)\n",
    "    gswil_model.fc1.requires_grad_(False)\n",
    "elif model_selection == 'cnn':\n",
    "    gswil_model = add_output_nodes(ckpt_file, arch='cnn', device=device)\n",
    "    gswil_model.conv_block1.requires_grad_(False)\n",
    "    gswil_model.conv_block2.requires_grad_(False)\n",
    "    gswil_model.conv_block3.Conv5.requires_grad_(False)\n",
    "    gswil_model.conv_block3.Relu5.requires_grad_(False)\n",
    "    gswil_model.conv_block3.BN5.requires_grad_(False)\n",
    "    \n",
    "gswil_model = gswil_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4bf4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "gswil_optimizer = torch.optim.Adam(gswil_model.parameters(), lr=initial_learning_rate)\n",
    "gswil_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=gswil_optimizer, gamma=decay_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e43776fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_sample_sizes = np.load('./data/fmnist_sim_scores_boot2.npy')\n",
    "sim_sample_sizes = np.array(sim_sample_sizes.tolist(),dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9652a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class GenDataset(Dataset):\n",
    "    \"\"\"All-purpose (FMNIST or CIFAR10) generated dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, data_name='img', class_name='label', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.files = self.landmarks_frame[data_name].values\n",
    "        self.data = self._load_data()\n",
    "        self.targets = self.landmarks_frame[class_name].values\n",
    "        \n",
    "    def _load_data(self):\n",
    "        #data = np.empty((len(self.files),28,28))\n",
    "        data = np.zeros([len(self.files),1,28,28])\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        for f in self.files:\n",
    "            img_name = os.path.join(self.root_dir, f)\n",
    "            pil_img = Image.open(img_name)\n",
    "            img = np.asarray(pil_img)\n",
    "            #plt.imshow(pil_img)\n",
    "            #plt.show()\n",
    "            \n",
    "            #img = torch.tensor(img)\n",
    "            #img1 = img / 2 + 0.5     # unnormalize\n",
    "            #npimg = img1.numpy()\n",
    "            #plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "            #plt.show()\n",
    "            \n",
    "            if self.transform is not None:\n",
    "                img = Image.open(img_name)\n",
    "                img = self.transform(img)\n",
    "                \n",
    "            #img2 = img / 2 + 0.5     # unnormalize\n",
    "            #npimg = img2.numpy()\n",
    "            #plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "            #plt.show()\n",
    "                \n",
    "            img = np.asarray(img)\n",
    "            #print(img[0][0])\n",
    "            \n",
    "            #np.append(data, img)\n",
    "            data[i,:,:,:] = img\n",
    "            i += 1\n",
    "            #if i == 10:\n",
    "            #    break\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #if torch.is_tensor(idx):\n",
    "        #    idx = idx.tolist()\n",
    "        \n",
    "        img, target = self.data[idx], self.targets[idx]\n",
    "        \n",
    "        # this is done for FMNIST, need for CIFAR10?\n",
    "        #img = Image.fromarray((img[:,:,:3] * [0.2989, 0.5870, 0.1140]).sum(axis=2), mode=\"L\")\n",
    "        \n",
    "        return torch.tensor(img, dtype=torch.float32), torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "537d378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_selection == 'fashionmnist':\n",
    "    transform = transforms.Compose([\n",
    "        #transforms.Resize(28),\n",
    "        #transforms.CenterCrop(28), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Normalize((0.5), (0.5))]) # Images are grayscale -> 1 channel\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "gfmnist_dataset_test = GenDataset(csv_file='data2/g_fashionmnist/annotations.csv',\n",
    "                                    root_dir='data2/g_fashionmnist/', data_name='filename', class_name='target', transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb85df9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n",
      "73\n",
      "325\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "gfmnist_dataset = gfmnist_dataset_test\n",
    "fmnist_classes = list(range(8)) # just get old classes from generated data\n",
    "\n",
    "class_subsets, old_class_idxs, subset_size = generate_dls(gfmnist_dataset, fmnist_classes)\n",
    "\n",
    "sampled_idxs = []\n",
    "for i in range(len(fmnist_classes)):\n",
    "    idx_sample = sample(old_class_idxs[i].tolist(), sim_sample_sizes[i])\n",
    "    sampled_idxs += idx_sample\n",
    "    \n",
    "#print(\"sampled_idxs\", sampled_idxs)\n",
    "    \n",
    "gswil_train_old_subset = torch.utils.data.Subset(gfmnist_dataset, sampled_idxs)\n",
    "\n",
    "gswil_new_subset, new_class_idx, subset_size = generate_dls(train_data, [9])\n",
    "new_class_idx_sample = sample(new_class_idx[0].tolist(), sim_sample_sizes[-1])\n",
    "gswil_train_new_subset = torch.utils.data.Subset(train_data, new_class_idx_sample)\n",
    "\n",
    "\n",
    "gswil_combined_subset = torch.utils.data.ConcatDataset([gswil_train_old_subset, gswil_train_new_subset])\n",
    "#gswil_combined_subset_test = torch.utils.data.ConcatDataset([gfmnist_dataset_test, gswil_train_new_subset])\n",
    "print(len(gswil_train_old_subset))\n",
    "print(len(gswil_train_new_subset))\n",
    "print(len(gswil_combined_subset))\n",
    "\n",
    "gswil_train_dl = torch.utils.data.DataLoader(gswil_combined_subset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "no_bag_test_idx = np.where((np.array(test_data.targets) != 8))[0]\n",
    "no_bag_test_subset = torch.utils.data.Subset(test_data, no_bag_test_idx)\n",
    "gswil_test_dl = torch.utils.data.DataLoader(no_bag_test_subset, batch_size=1, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4aa9de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, device, swap=False, swap_labels=[], classes = 9) -> float:\n",
    "    '''\n",
    "        Model test loop. Performs a single epoch of model updates.\n",
    "\n",
    "        * USAGE *\n",
    "        Within a training loop of range(num_epochs) to perform epoch validation, or after training to perform testing.\n",
    "\n",
    "        * PARAMETERS *\n",
    "        dataloader: A torch.utils.data.DataLoader object\n",
    "        model: A torch model which subclasses torch.nn.Module\n",
    "        loss_fn: A torch loss function, such as torch.nn.CrossEntropyLoss\n",
    "        optimizer: A torch.optim optimizer\n",
    "        device: 'cuda' or 'cpu'\n",
    "\n",
    "        * RETURNS *\n",
    "        float: The average test loss\n",
    "    '''\n",
    "\n",
    "    # TODO: can the swap stuff be removed now?\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    y_pred_list, targets = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            if swap:\n",
    "                for i in range(len(y)):\n",
    "                    if y[i] == swap_labels[0]:\n",
    "                        y[i] = swap_labels[1]\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            targets.append(y.tolist())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            _, y_pred_tags = torch.max(pred, dim=1)\n",
    "            y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "            \n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "    y_pred_list = [item for sublist in y_pred_list for item in sublist]\n",
    "    \n",
    "    targets = [item for sublist in targets for item in sublist]\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss, np.asarray(y_pred_list), np.asarray(targets), 100*correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7689e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size 784\n",
      "num_outputs 9\n"
     ]
    }
   ],
   "source": [
    "if model_selection == 'linear':\n",
    "    swil_model = add_output_nodes(ckpt_file, arch='linear', device=device)\n",
    "    swil_model.input_layer.requires_grad_(False)\n",
    "elif model_selection == 'cnn-demo':\n",
    "    swil_model = add_output_nodes(ckpt_file, arch='cnn-demo')\n",
    "    swil_model.conv1.requires_grad_(False)\n",
    "    swil_model.conv2.requires_grad_(False)\n",
    "    swil_model.fc1.requires_grad_(False)\n",
    "elif model_selection == 'cnn6':\n",
    "    swil_model = add_output_nodes(ckpt_file, device, arch='cnn')\n",
    "    swil_model.conv_block1.requires_grad_(False)\n",
    "    swil_model.conv_block2.requires_grad_(False)\n",
    "    swil_model.conv_block3.Conv5.requires_grad_(False)\n",
    "    swil_model.conv_block3.Relu5.requires_grad_(False)\n",
    "    swil_model.conv_block3.BN5.requires_grad_(False)    \n",
    "    \n",
    "swil_model = swil_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e92213e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23 28 21 24 21 20 56 59 73]\n",
      "0\n",
      "input_size 784\n",
      "num_outputs 9\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/loganbecker/opt/anaconda3/envs/dalleenv/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/loganbecker/opt/anaconda3/envs/dalleenv/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'GenDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/82/015rpstj7z1_2l244gtnbyvm0000gn/T/ipykernel_28981/1780801819.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgswil_train_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgswil_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgswil_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_actual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_gswil_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgswil_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m#train_losses.append(train_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/School/Austin/C-S394-NN/CS394N/src/utils/model_tools.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer, device, swap, swap_labels, flag)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dalleenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dalleenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dalleenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dalleenv/lib/python3.10/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dalleenv/lib/python3.10/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dalleenv/lib/python3.10/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mForkServerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dalleenv/lib/python3.10/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dalleenv/lib/python3.10/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dalleenv/lib/python3.10/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mfds_to_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ntrials = 10\n",
    "#swil_optimizer = torch.optim.Adam(swil_model.parameters(), lr=initial_learning_rate)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=swil_optimizer, gamma=decay_rate)\n",
    "\n",
    "#fmnist_classes = list(range(8)) + [9]\n",
    "\n",
    "#with open(r'./data/fmnist_sim_scores_boot.txt', 'r') as fp:\n",
    "#    sim_scores = [float(i) for i in fp.readlines()]\n",
    "\n",
    "#sim_sum = sum(sim_scores)\n",
    "#sim_norms = [x/sim_sum for x in sim_scores]\n",
    "\n",
    "#boots_sample_size = 75\n",
    "#sim_sample_sizes = [27 if x < 0.2 else int(x * boots_sample_size*3.52) for x in sim_norms] + [75]\n",
    "\n",
    "#sim_sample_sizes = np.load('./data/cifar10_sim_scores_cat.npy')\n",
    "#sim_sample_sizes = np.array(sim_sample_sizes.tolist(),dtype='uint8')\n",
    "sim_sample_sizes = np.load('./data/fmnist_sim_scores_boot2.npy')\n",
    "sim_sample_sizes = np.array(sim_sample_sizes.tolist(),dtype='uint8')\n",
    "#sim_sample_sizes = np.zeros(9); sim_sample_sizes[-1] = 6000;\n",
    "#sim_sample_sizes = sim_sample_sizes.astype(int)\n",
    "#sim_sample_sizes = np.array(sim_sample_sizes.tolist(),dtype='uint8')\n",
    "print(sim_sample_sizes)\n",
    "\n",
    "from random import sample\n",
    "num_epochs = 15\n",
    "for nt in range(ntrials):\n",
    "    print(nt)\n",
    "    \n",
    "    if model_selection == 'linear':\n",
    "        gswil_model = add_output_nodes(ckpt_file, arch='linear', device=device)\n",
    "        gswil_model.input_layer.requires_grad_(False)\n",
    "    elif model_selection == 'cnn-demo':\n",
    "        swil_model = add_output_nodes(ckpt_file, arch='cnn-demo')\n",
    "        swil_model.conv1.requires_grad_(False)\n",
    "        swil_model.conv2.requires_grad_(False)\n",
    "        swil_model.fc1.requires_grad_(False)\n",
    "    elif model_selection == 'cnn6':\n",
    "        swil_model = add_output_nodes(ckpt_file, device, arch='cnn')\n",
    "        swil_model.conv_block1.requires_grad_(False)\n",
    "        swil_model.conv_block2.requires_grad_(False)\n",
    "        swil_model.conv_block3.Conv5.requires_grad_(False)\n",
    "        swil_model.conv_block3.Relu5.requires_grad_(False)\n",
    "        swil_model.conv_block3.BN5.requires_grad_(False) \n",
    "\n",
    "    gswil_model = swil_model.to(device)\n",
    "    gswil_optimizer = torch.optim.Adam(gswil_model.parameters(), lr=initial_learning_rate)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=gswil_optimizer, gamma=decay_rate)\n",
    "\n",
    "    gfmnist_dataset = gfmnist_dataset_test\n",
    "    fmnist_classes = list(range(8)) # just get old classes from generated data\n",
    "\n",
    "    class_subsets, old_class_idxs, subset_size = generate_dls(gfmnist_dataset, fmnist_classes)\n",
    "\n",
    "    sampled_idxs = []\n",
    "    for i in range(len(fmnist_classes)):\n",
    "        idx_sample = sample(old_class_idxs[i].tolist(), sim_sample_sizes[i])\n",
    "        sampled_idxs += idx_sample\n",
    "\n",
    "    gswil_train_old_subset = torch.utils.data.Subset(gfmnist_dataset, sampled_idxs)\n",
    "\n",
    "    gswil_new_subset, new_class_idx, subset_size = generate_dls(train_data, [9])\n",
    "    new_class_idx_sample = sample(new_class_idx[0].tolist(), sim_sample_sizes[-1])\n",
    "    gswil_train_new_subset = torch.utils.data.Subset(train_data, new_class_idx_sample)\n",
    "\n",
    "    gswil_combined_subset = torch.utils.data.ConcatDataset([gswil_train_old_subset, gswil_train_new_subset])\n",
    "    gswil_train_dl = torch.utils.data.DataLoader(gswil_combined_subset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "    no_bag_test_idx = np.where((np.array(test_data.targets) != 8))[0]\n",
    "    no_bag_test_subset = torch.utils.data.Subset(test_data, no_bag_test_idx)\n",
    "    gswil_test_dl = torch.utils.data.DataLoader(no_bag_test_subset, batch_size=1, shuffle=True, num_workers=2)\n",
    "        \n",
    "    weight_dir = './logs/'\n",
    "    model_file = weight_dir + model_selection + '_' + dataset_selection + '_' + 'holdout' + '_' + '[8]' + '_gswil' + str(nt) + '.pt'\n",
    "    recall_file = weight_dir + model_selection + '_' + dataset_selection + '_' + 'holdout' + '_' + '[8]' + '_gswil_recall'+ str(nt) + '.npy'\n",
    "    #train_losses_file = weight_dir + model_selection + '_' + dataset_selection + '_' + 'holdout' + '_' + '[8]' + '_swil_train_loss0.txt'\n",
    "    test_losses_file = weight_dir + model_selection + '_' + dataset_selection + '_' + 'holdout' + '_' + '[8]' + 'gswil_test_loss' + str(nt) + '.txt'\n",
    "    accuracies_file = weight_dir + model_selection + '_' + dataset_selection + '_' + 'holdout' + '_' + '[8]' + 'gswil_accuracies' + str(nt) + '.txt'\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    #t = trange(num_epochs)\n",
    "    t = range(num_epochs)\n",
    "    y_preds = []\n",
    "    y_actuals = []\n",
    "    accuracies = []\n",
    "\n",
    "    for epoch in t:\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        train_loss = train(gswil_train_dl, gswil_model, loss_fn, gswil_optimizer, device, swap=True, swap_labels=[9,8])\n",
    "        test_loss, y_pred, y_actual, acc = test(test_gswil_dl, gswil_model, loss_fn, device, swap=True, swap_labels=[9,8])\n",
    "        #train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        y_preds.append(y_pred)\n",
    "        y_actuals.append(y_actual)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "        #t.set_description(f\"Epoch {epoch} train loss: {epoch_loss_train[-1]:.3f}\")\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    torch.save(swil_model.state_dict(), model_file)\n",
    "\n",
    "    recalls = get_recall_per_epoch(y_actuals, y_preds, 9)\n",
    "    np.save(recall_file, recalls)\n",
    "\n",
    "    #with open(train_losses_file, 'w') as fp:\n",
    "    #    for s in train_losses:\n",
    "    #        fp.write(\"%s\\n\" % s)\n",
    "\n",
    "    with open(test_losses_file, 'w') as fp:\n",
    "        for x in test_losses:\n",
    "            fp.write(\"%s\\n\" % x)\n",
    "\n",
    "    with open(accuracies_file, 'w') as fp:\n",
    "        for x in accuracies:\n",
    "            fp.write(\"%s\\n\" % x)\n",
    "\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2cf111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
